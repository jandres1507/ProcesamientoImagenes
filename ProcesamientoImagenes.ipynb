{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976247e2",
   "metadata": {},
   "source": [
    "# PROYECTO DE MATERIA \n",
    "## TRATAMIENTO DE DATOS\n",
    "\n",
    "Jaime Andrés Subía"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ea583",
   "metadata": {},
   "source": [
    "### Paso 1: ###\n",
    "Importamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3307ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D, MaxPooling2D \n",
    "from keras.layers import Activation, Dropout, Flatten, Dense \n",
    "from keras import backend as K "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b251c373",
   "metadata": {},
   "source": [
    "### Paso 2: ###\n",
    "Seteamos el tamano de las imagenes a ser tratadas (224*224)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b54823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 224, 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47e6da8",
   "metadata": {},
   "source": [
    "### Paso 3: ###\n",
    "Inicializamos las variables para paths locales donde se encuentran los DataSets a ser procesados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55713d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_train_carnes = 'C:/Users/JANDRES/Documents/CarneDataset/train'\n",
    "dir_test_carnes = 'C:/Users/JANDRES/Documents/CarneDataset/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae942d",
   "metadata": {},
   "source": [
    "### Paso 4:\n",
    "Insertamos la cantidad de imagenes a procesar, tanto de entrenamiento como de test y las variables para setear la cantidad de veces que el modelo sera entrenado (epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed64d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_img = 1633\n",
    "num_test_img = 810\n",
    "epochs = 30\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b2f5fb",
   "metadata": {},
   "source": [
    "### Paso 5: \n",
    "Verificamos el formato de datos, el modelo verificará primero y luego la forma de entrada se alimentará en consecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb3f2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first': \n",
    "    input_shape = (3, img_width, img_height) \n",
    "else: \n",
    "    input_shape = (img_width, img_height, 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7532c59a",
   "metadata": {},
   "source": [
    "### Paso 6: \n",
    "Utilizamos las siguientes funciones:\n",
    "##### Conv2D #####\n",
    "Capa para convolucionar la imagen en múltiples imágenes \n",
    "##### Activation #####\n",
    "Es la función de activación. \n",
    "##### MaxPooling2D #####\n",
    "Se usa para agrupar al máximo el valor de la matriz de tamaño dada y lo mismo se usa para las siguientes 2 capas. \n",
    "##### Flatten #####\n",
    "Se usa para aplanar las dimensiones de la imagen obtenida después de convolucionarla.\n",
    "##### Dense ###### \n",
    "Se usa para hacer de este un modelo completamente conectado y es la capa oculta. \n",
    "##### Dropout #####\n",
    "Se utiliza para evitar el sobreajuste en el conjunto de datos. \n",
    "##### Dense ###### \n",
    "Finalmente reutilizamos DENSE para la capa de salida que contiene solo una neurona que decide a qué categoría pertenece la imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f946c890",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "model.add(Conv2D(32, (2, 2), input_shape=input_shape)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed617cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (2, 2))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size = (2, 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00818654",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (2, 2))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(MaxPooling2D(pool_size = (2, 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9b14903",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten()) \n",
    "model.add(Dense(64)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(1)) \n",
    "model.add(Activation('sigmoid')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af9d643",
   "metadata": {},
   "source": [
    "### Paso 7: \n",
    "Utilizamos la función de compilación que implica el uso de pérdida, optimizadores y métricas. La función de pérdida utilizada es binary_crossentropy, el optimizador utilizado es rmsprop ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e01aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ec5dc",
   "metadata": {},
   "source": [
    "### Paso 8:\n",
    "Implementamos el generado de datos ImageDataGenerator con las siguientes funciones:\n",
    "###### ImageDataGenerator \n",
    "Cambia la escala de la imagen, aplica corte en algún rango, hace zoom en la imagen y realiza un volteo horizontal con la imagen. Este ImageDataGenerator incluye todas las orientaciones posibles de la imagen. \n",
    "##### train_datagen.flow_from_directory \n",
    "Función que se usa para preparar datos del directorio train_dataset Target_size especifica el tamaño objetivo de la imagen. \n",
    "##### test_datagen.flow_from_directory \n",
    "Se usa para preparar datos de prueba para el modelo y todo es similar al anterior. \n",
    "##### fit_generator \n",
    "Se utiliza para ajustar los datos en el modelo creado anteriormente, otros factores utilizados son steps_per_epochs que nos informan sobre la cantidad de veces que se ejecutará el modelo para los datos de entrenamiento. \n",
    "##### epochs\n",
    "Nos informa la cantidad de veces que el modelo será entrenado en pase hacia adelante y hacia atrás. \n",
    "##### Validation_data \n",
    "Se utiliza para alimentar los datos de validación/prueba en el modelo. \n",
    "##### validation_steps \n",
    "Indica el número de muestras de validación/prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dac662c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1633 images belonging to 8 classes.\n",
      "Found 810 images belonging to 8 classes.\n",
      "Epoch 1/10\n",
      "25/25 [==============================] - 15s 549ms/step - loss: 0.7617 - accuracy: 0.6200 - val_loss: 0.3592 - val_accuracy: 0.9062\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 15s 600ms/step - loss: 0.4773 - accuracy: 0.7975 - val_loss: 0.2833 - val_accuracy: 0.9062\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 15s 597ms/step - loss: 0.4416 - accuracy: 0.8250 - val_loss: 0.2877 - val_accuracy: 0.9062\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 16s 652ms/step - loss: 0.4349 - accuracy: 0.8325 - val_loss: 0.2843 - val_accuracy: 0.8854\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 17s 657ms/step - loss: 0.3632 - accuracy: 0.8600 - val_loss: 0.2483 - val_accuracy: 0.8854\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 16s 627ms/step - loss: 0.3615 - accuracy: 0.8550 - val_loss: 0.2958 - val_accuracy: 0.8542\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 15s 582ms/step - loss: 0.3466 - accuracy: 0.8625 - val_loss: 0.2691 - val_accuracy: 0.8750\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 14s 569ms/step - loss: 0.2743 - accuracy: 0.8800 - val_loss: 0.3147 - val_accuracy: 0.8542\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 14s 557ms/step - loss: 0.2883 - accuracy: 0.8800 - val_loss: 0.3288 - val_accuracy: 0.8438\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 14s 563ms/step - loss: 0.2854 - accuracy: 0.8825 - val_loss: 0.3703 - val_accuracy: 0.8021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a6ab64a2e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator( \n",
    "    rescale = 1./ 255, \n",
    "    shear_range = 0.2, \n",
    "    zoom_range = 0.2, \n",
    "    horizontal_flip = True) \n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1. / 255) \n",
    "  \n",
    "train_generator = train_datagen.flow_from_directory( \n",
    "    dir_train_carnes, \n",
    "    target_size = (img_width, img_height), \n",
    "    batch_size = batch_size, \n",
    "    class_mode = 'binary') \n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory( \n",
    "    dir_test_carnes, \n",
    "    target_size = (img_width, img_height), \n",
    "    batch_size = batch_size, \n",
    "    class_mode = 'binary') \n",
    "  \n",
    "model.fit( \n",
    "    train_generator, \n",
    "    steps_per_epoch = num_train_img // batch_size, \n",
    "    epochs = epochs, \n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = num_test_img // batch_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015cefda",
   "metadata": {},
   "source": [
    "### Nota \n",
    "Se valida en la ejecución del modelo las imagenes procesadas y las 8 clases existentes por cada tipo de ellas. El procesamiento tardó al rededor de 5 minutos en la ejecución del entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258f60e",
   "metadata": {},
   "source": [
    "### Paso 9:\n",
    "Guardamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c58e048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelo_entrenamiento.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe47ba3",
   "metadata": {},
   "source": [
    "### Paso 10:\n",
    "Importamos librerias necesarias para cargar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be0b401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import numpy as np\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e502d",
   "metadata": {},
   "source": [
    "### Paso 11:\n",
    "Cargamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a065092",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('modelo_entrenamiento.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ba2a8",
   "metadata": {},
   "source": [
    "### Paso 12:\n",
    "Cargamos la imagen a ser procesada tomada desde la carpeta de TEST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d71ddf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step\n",
      "Clase a la que petenece (CLASS_01, CLASS_02, CLASS_03, CLASS_04, CLASS_05, CLASS_06, CLASS_07, CLASS_08):  5\n"
     ]
    }
   ],
   "source": [
    "image = load_img('C:/Users/JANDRES/Documents/CarneDataset/test/CLASS_05/07-CAPTURE_20220614_074648_771.png', target_size=(224, 224)) \n",
    "img = np.array(image) \n",
    "img = img / 255.0\n",
    "img = img.reshape(1,224,224,3) \n",
    "label = model.predict(img) \n",
    "print(\"Clase a la que petenece (CLASS_01, CLASS_02, CLASS_03, CLASS_04, CLASS_05, CLASS_06, CLASS_07, CLASS_07, CLASS_08): \", label[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcde71d",
   "metadata": {},
   "source": [
    "### Conclusión:\n",
    "Podemos verificar que la clase cargada es la número 5 después del entrenamiento al modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
